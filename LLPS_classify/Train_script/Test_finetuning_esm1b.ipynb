{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../dataset/dataset2.0/\" # Path to data\n",
    "EMB_PRE_PATH = \"/home/bli/.cache/torch/hub/checkpoints/esm1b_t33_650M_UR50S.pt\" \n",
    "EMBED_PATH =DATA_PATH+'ESM_finetune_embed/'\n",
    "EMB_LAYER = 33\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn \n",
    "from esm import Alphabet, FastaBatchedDataset, ProteinBertModel, pretrained, MSATransformer\n",
    "import utils\n",
    "from model import LayerNormNet\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve,f1_score,recall_score,precision_score,matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinExtractionParams:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_location=EMB_PRE_PATH,\n",
    "        fasta_file = None,\n",
    "        csv_file = None,\n",
    "        output_dir = None,\n",
    "        toks_per_batch = 1024,\n",
    "        repr_layers=[33],\n",
    "        include='mean',\n",
    "        truncation_seq_length = 512,\n",
    "        nogpu=False,\n",
    "    ):\n",
    "        self.model_location = model_location\n",
    "        self.fasta_file = fasta_file\n",
    "        self.csv_file = csv_file\n",
    "\n",
    "\n",
    "        self.toks_per_batch = toks_per_batch\n",
    "        self.repr_layers = repr_layers\n",
    "        self.include = include\n",
    "        self.truncation_seq_length = truncation_seq_length\n",
    "        self.nogpu = nogpu\n",
    "args = ProteinExtractionParams(output_dir='./ESM_finetuning/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fine_tunning_esm(nn.Module):\n",
    "  def __init__(self, n_classes=2,args = None):\n",
    "    super(fine_tunning_esm, self).__init__()\n",
    "    self.esm1b,self.alphabet = pretrained.load_model_and_alphabet(args.model_location)\n",
    "    self.drop1 = nn.Dropout(p=0.2)\n",
    "    self.drop2 = nn.Dropout(p=0.5)\n",
    "    self.drop3 = nn.Dropout(p=0.2)\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "    self.linear = nn.Linear(1280, n_classes)\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "  def forward(self, strs,toks):\n",
    "    return_contacts = \"contacts\" in args.include\n",
    "    out = self.esm1b(toks, repr_layers=args.repr_layers, return_contacts=return_contacts)\n",
    "\n",
    "    representations = {\n",
    "        layer: t for layer, t in out[\"representations\"].items()\n",
    "    }\n",
    "    if return_contacts:\n",
    "        contacts = out[\"contacts\"].to(device=\"cpu\") \n",
    "    x = []   \n",
    "    for i, tok in enumerate(toks):\n",
    "        result = {}\n",
    "        truncate_len = min(args.truncation_seq_length, len(strs[i]))\n",
    "        if \"per_tok\" in args.include:\n",
    "            result[\"representations\"] = {\n",
    "                layer: t[i, 1 : truncate_len + 1].clone()\n",
    "                for layer, t in representations.items()\n",
    "            }\n",
    "        if \"mean\" in args.include:\n",
    "            result[\"mean_representations\"] = {\n",
    "                layer: t[i, 1 : truncate_len + 1].mean(0).clone()\n",
    "                for layer, t in representations.items()\n",
    "            }\n",
    "        if \"bos\" in args.include:\n",
    "            result[\"bos_representations\"] = {\n",
    "                layer: t[i, 0].clone() for layer, t in representations.items()\n",
    "            }\n",
    "        if return_contacts:\n",
    "            result[\"contacts\"] = contacts[i, : truncate_len, : truncate_len].clone()\n",
    "        x.append(result['mean_representations'][33])\n",
    "    x = torch.vstack((*x,))\n",
    "    out = self.linear(x)\n",
    "    return out\n",
    "class fine_tunning_esm_LN(nn.Module):\n",
    "    \n",
    "  def __init__(self,args = None):\n",
    "    super(fine_tunning_esm_LN, self).__init__()\n",
    "    self.esm1b,self.alphabet = pretrained.load_model_and_alphabet(args.model_location)\n",
    "    self.model = LayerNormNet(hidden_dim=512,out_dim=2)\n",
    "\n",
    "  def forward(self, strs,toks):\n",
    "    return_contacts = \"contacts\" in args.include\n",
    "    out = self.esm1b(toks, repr_layers=args.repr_layers, return_contacts=return_contacts)\n",
    "\n",
    "    representations = {\n",
    "        layer: t for layer, t in out[\"representations\"].items()\n",
    "    }\n",
    "    if return_contacts:\n",
    "        contacts = out[\"contacts\"].to(device=\"cpu\") \n",
    "    x = []   \n",
    "    for i, tok in enumerate(toks):\n",
    "        result = {}\n",
    "        truncate_len = min(args.truncation_seq_length, len(strs[i]))\n",
    "        if \"per_tok\" in args.include:\n",
    "            result[\"representations\"] = {\n",
    "                layer: t[i, 1 : truncate_len + 1].clone()\n",
    "                for layer, t in representations.items()\n",
    "            }\n",
    "        if \"mean\" in args.include:\n",
    "            result[\"mean_representations\"] = {\n",
    "                layer: t[i, 1 : truncate_len + 1].mean(0).clone()\n",
    "                for layer, t in representations.items()\n",
    "            }\n",
    "        if \"bos\" in args.include:\n",
    "            result[\"bos_representations\"] = {\n",
    "                layer: t[i, 0].clone() for layer, t in representations.items()\n",
    "            }\n",
    "        if return_contacts:\n",
    "            result[\"contacts\"] = contacts[i, : truncate_len, : truncate_len].clone()\n",
    "        x.append(result['mean_representations'][33])\n",
    "    x = torch.vstack((*x,))\n",
    "    out = self.model(x)\n",
    "    return out\n",
    "class ESM_CNN1d(nn.Module):\n",
    "  def __init__(self,kernel_sizes = [3, 4, 5], n_classes=2,args = None,num_channels = [512, 512, 512]):\n",
    "    super(ESM_CNN1d, self).__init__()\n",
    "    self.esm1b,self.alphabet = pretrained.load_model_and_alphabet(args.model_location)\n",
    "    self.dropout = nn.Dropout(0.5)\n",
    "    self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "    self.relu = nn.ReLU()\n",
    "    # 创建多个一维卷积层\n",
    "    self.convs = nn.ModuleList()\n",
    "    for c, k in zip(num_channels, kernel_sizes):\n",
    "        self.convs.append(nn.Conv1d(1280, c, k))\n",
    "    self.relu = nn.ReLU()\n",
    "    self.decoder = nn.Linear(sum(num_channels), n_classes)\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "  def forward(self, strs,toks):\n",
    "    return_contacts = \"contacts\" in args.include\n",
    "    out = self.esm1b(toks, repr_layers=args.repr_layers, return_contacts=return_contacts)\n",
    "    embeddings = out[\"representations\"][33]\n",
    "    embeddings = embeddings.permute(0, 2, 1)\n",
    "    encoding = torch.cat([\n",
    "        torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1)\n",
    "        for conv in self.convs], dim=1)\n",
    "    outputs = self.decoder(self.dropout(encoding))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testDataloader(test_df,alphabet,args):\n",
    "    test_dataset = FastaBatchedDataset(test_df['label'],test_df['seq'])\n",
    "    test_batches = test_dataset.get_batch_indices(args.toks_per_batch, extra_toks_per_seq=1)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, collate_fn=alphabet.get_batch_converter(args.truncation_seq_length), batch_sampler=test_batches\n",
    "    )\n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):  #@save\n",
    "    \"\"\"计算预测正确的数量\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "def evaluate_accuracy_gpu(net, data_iter, devices):\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度\n",
    "\n",
    "    Defined in :numref:`sec_lenet`\"\"\"\n",
    "    net.eval()  # 设置为评估模式\n",
    "    # 正确预测的数量，总预测的数量\n",
    "    metric = utils.Accumulator(2)\n",
    "    pred_y = []\n",
    "    y = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (labels, strs, toks) in enumerate(data_iter):\n",
    "            labels = torch.tensor(labels)\n",
    "            labels = labels.to(devices[0])\n",
    "            toks = toks.to(devices[0])\n",
    "            y_hat = net(strs,toks)\n",
    "            metric.add(accuracy(y_hat, labels), labels.numel())\n",
    "            \n",
    "            y_hat = y_hat.argmax(axis=1)\n",
    "            pred_y.append(y_hat)\n",
    "            y.append(labels)\n",
    "\n",
    "    y = torch.cat(y,dim=0)\n",
    "    y_hat =  torch.cat(pred_y,dim=0)\n",
    "    acc = metric[0] / metric[1]\n",
    "    return y,y_hat,acc\n",
    "\n",
    "def evaluate(true_y,pred_y):\n",
    "    # 计算评估指标\n",
    "    true_y = true_y.cpu()\n",
    "    pred_y = pred_y.cpu()\n",
    "    accuracy = accuracy_score(true_y, pred_y)\n",
    "    mcc = matthews_corrcoef(true_y, pred_y)\n",
    "    roc_auc = roc_auc_score(true_y, pred_y)\n",
    "    f1 = f1_score(true_y, pred_y)\n",
    "    recall = recall_score(true_y, pred_y)\n",
    "    precision = precision_score(true_y,pred_y)\n",
    "\n",
    "    # 将评估结果转换为字典\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'mcc' :mcc,\n",
    "        'roc_auc': roc_auc,\n",
    "        'f1_score': f1,\n",
    "        'recall_score': recall,\n",
    "        'precision':precision\n",
    "    }\n",
    "\n",
    "    # # 打印字典结果\n",
    "    print(results)\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net,alphabet,test_csv ='test_data1_2023-6-9_15_31.csv', devices = utils.try_all_gpus()):\n",
    "    print('start test---------------->>>>>>>>>>')\n",
    "    test_df = pd.read_csv(DATA_PATH+test_csv)\n",
    "    test_dataloader = get_testDataloader(test_df=test_df,alphabet=alphabet,args=args)\n",
    "    y,y_hat,acc = evaluate_accuracy_gpu(net,test_dataloader,devices=devices)\n",
    "    print(acc)\n",
    "    \n",
    "    evaluate(true_y=y,pred_y=y_hat)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "esm1b,alphabet = pretrained.load_model_and_alphabet(args.model_location)\n",
    "net = fine_tunning_esm_LN(args=args)\n",
    "path_checkpoint = \"./ESM_finetuning/checkpoint_esm2_2.0/bestmodel.pkl\"  # 模型权重路径\n",
    "checkpoint = torch.load(path_checkpoint)  # 加载权重\n",
    "net.load_state_dict(checkpoint['net'])  # 加载模型可学习参数\n",
    "net.cuda()\n",
    "trainer = torch.optim.Adam([{\"params\": net.esm1b.parameters(), \"lr\": 1e-7}],\n",
    "                            lr = 1e-7, weight_decay=1e-3)\n",
    "trainer.load_state_dict(checkpoint['optimizer'])  # 加载优化器参数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start test---------------->>>>>>>>>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.708029197080292\n",
      "{'accuracy': 0.708029197080292, 'mcc': 0.46829651968447306, 'roc_auc': 0.6788172043010753, 'f1_score': 0.5348837209302326, 'recall_score': 0.3709677419354839, 'precision': 0.9583333333333334}\n"
     ]
    }
   ],
   "source": [
    "test(net=net,alphabet=alphabet,test_csv='test_data2_2023-6-9_15_31.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class f_esm(nn.Module):\n",
    "  def __init__(self, n_classes=2,args = None):\n",
    "    super(f_esm, self).__init__()\n",
    "    self.esm1b,self.alphabet = pretrained.load_model_and_alphabet(args.model_location)\n",
    "    self.drop1 = nn.Dropout(p=0.2)\n",
    "    self.drop2 = nn.Dropout(p=0.5)\n",
    "    self.drop3 = nn.Dropout(p=0.2)\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "    self.linear = nn.Linear(1280, n_classes)\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "  def forward(self, strs,toks):\n",
    "    return_contacts = \"contacts\" in args.include\n",
    "    esm_out = self.esm1b(toks, repr_layers=args.repr_layers, return_contacts=return_contacts)\n",
    "\n",
    "    representations = {\n",
    "        layer: t for layer, t in esm_out[\"representations\"].items()\n",
    "    }\n",
    "    if return_contacts:\n",
    "        contacts = esm_out[\"contacts\"].to(device=\"cpu\") \n",
    "    x = []   \n",
    "    for i, tok in enumerate(toks):\n",
    "        result = {}\n",
    "        truncate_len = min(args.truncation_seq_length, len(strs[i]))\n",
    "        if \"per_tok\" in args.include:\n",
    "            result[\"representations\"] = {\n",
    "                layer: t[i, 1 : truncate_len + 1].clone()\n",
    "                for layer, t in representations.items()\n",
    "            }\n",
    "        if \"mean\" in args.include:\n",
    "            result[\"mean_representations\"] = {\n",
    "                layer: t[i, 1 : truncate_len + 1].mean(0).clone()\n",
    "                for layer, t in representations.items()\n",
    "            }\n",
    "        if \"bos\" in args.include:\n",
    "            result[\"bos_representations\"] = {\n",
    "                layer: t[i, 0].clone() for layer, t in representations.items()\n",
    "            }\n",
    "        if return_contacts:\n",
    "            result[\"contacts\"] = contacts[i, : truncate_len, : truncate_len].clone()\n",
    "        x.append(result['mean_representations'][33])\n",
    "    x = torch.vstack((*x,))\n",
    "    out = self.linear(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finetune_embed(net, data_iter, devices):\n",
    "    net.eval()  # 设置为评估模式\n",
    "    embedding = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (labels, strs, toks) in enumerate(data_iter):\n",
    "\n",
    "            toks = toks.to(devices[0])\n",
    "            embed = net(strs,toks)\n",
    "            embed = embed.detach().cpu().numpy()\n",
    "            \n",
    "            embedding.append(embed)\n",
    "\n",
    "    embedding = np.vstack((*embedding,))\n",
    "    print(embedding.shape)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embed(data_file,net):\n",
    "    input_data  =DATA_PATH+data_file\n",
    "    output_dir = EMBED_PATH+data_file.split('.')[0]\n",
    "    test_df = pd.read_csv(input_data)\n",
    "    dataloader = get_testDataloader(test_df=test_df,alphabet=alphabet,args=args)\n",
    "    embeds = get_finetune_embed(net=net,data_iter = dataloader,devices= utils.try_all_gpus())\n",
    "    np.save(output_dir+'_embeds.npy', embeds)\n",
    "    print('Extract ESM fine_tuning embeddings for {}, save in {}'.format(input_data,output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = f_esm(args = args)\n",
    "# path_checkpoint = \"./ESM_finetuning/checkpoint_esm2_2.0/bestmodel.pkl\"  # 模型权重路径\n",
    "# checkpoint = torch.load(path_checkpoint)  # 加载权重\n",
    "# net.load_state_dict(checkpoint['net'])  # 加载模型可学习参数\n",
    "# net.cuda()\n",
    "# for file in os.listdir(DATA_PATH):\n",
    "#     if file.endswith('.csv'):\n",
    "#         print(file)\n",
    "#         extract_embed(file,net)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.decomposition import PCA\n",
    "# def load_embed(csv_file):\n",
    "#     Embed_PATH = EMBED_PATH+csv_file.split('.')[0]+'_embeds.npy'\n",
    "#     data_df =  pd.read_csv(DATA_PATH+csv_file)\n",
    "#     ys = data_df['label']\n",
    "#     Xs = np.load(Embed_PATH)\n",
    "#     print('load{} esm finetuning embedding from {}'.format(csv_file,Embed_PATH))\n",
    "#     print(len(ys))\n",
    "#     print(Xs.shape)\n",
    "#     return Xs,ys\n",
    "# Xs = []\n",
    "# ys = []\n",
    "# for file in os.listdir(DATA_PATH):\n",
    "#     if file.endswith('.csv'):\n",
    "#         if file.endswith('llpsdb_d.csv'):continue\n",
    "#         x,y = load_embed(file)\n",
    "#         Xs.append(x)\n",
    "#         ys.append(y)\n",
    "# Xs = np.vstack((*Xs,))\n",
    "# ys = [y for sub in ys for y in sub]\n",
    "# print(len(ys))\n",
    "# print(Xs.shape)\n",
    "# num_pca_components = 2\n",
    "# pca = PCA(num_pca_components)\n",
    "# Xs_pca = pca.fit_transform(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig_dims = (7, 6)\n",
    "# fig, ax = plt.subplots(figsize=fig_dims)\n",
    "# sc = ax.scatter(Xs_pca[:,0], Xs_pca[:,1], c=ys, marker='.')\n",
    "# ax.set_xlabel('ESM finetuning PCA first principal component')\n",
    "# ax.set_ylabel('ESM finetuning PCA second principal component')\n",
    "# plt.colorbar(sc, label='LLPS tendency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Xihe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
