{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "import time\n",
    "from data_utlis.dataset import FastConvert\n",
    "from models.model import SentenceVAE\n",
    "from utils import to_var, idx2word, interpolate\n",
    "from torch.utils.data import DataLoader\n",
    "from screening_process.screen import screen\n",
    "def main(args):\n",
    "    \n",
    "    seqs = ['KKRPKPGGWNTGGSRYPGQGSPGGNRYPPQGGGGWGQPHGGGWGQPHGGGWGQPHGGGWGQPHGGGWGQGGGTHSQWNKPSKPKTNMKHMAGAAAAGAVVGGLGGYMLGSAMSRPIIHFGSD',\n",
    "       'SKGPGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGY'\n",
    "       ]\n",
    "    with open(args.data_dir+'/vocab.json', 'r') as file:\n",
    "        vocab = json.load(file)\n",
    "\n",
    "    w2i, i2w = vocab['w2i'], vocab['i2w']\n",
    "\n",
    "    model = SentenceVAE(\n",
    "        vocab_size=len(w2i),\n",
    "        sos_idx=w2i['<sos>'],\n",
    "        eos_idx=w2i['<eos>'],\n",
    "        pad_idx=w2i['<pad>'],\n",
    "        unk_idx=w2i['<unk>'],\n",
    "        max_sequence_length=args.max_sequence_length,\n",
    "        embedding_size=args.embedding_size,\n",
    "        rnn_type=args.rnn_type,\n",
    "        hidden_size=args.hidden_size,\n",
    "        word_dropout=args.word_dropout,\n",
    "        embedding_dropout=args.embedding_dropout,\n",
    "        latent_size=args.latent_size,\n",
    "        num_layers=args.num_layers,\n",
    "        bidirectional=args.bidirectional\n",
    "        )\n",
    "\n",
    "    if not os.path.exists(args.load_checkpoint):\n",
    "        raise FileNotFoundError(args.load_checkpoint)\n",
    "\n",
    "    model.load_state_dict(torch.load(args.load_checkpoint))\n",
    "    print(\"Model loaded from %s\" % args.load_checkpoint)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # samples, z = model.inference(n=args.num_samples)\n",
    "    # print('----------SAMPLES----------')\n",
    "    # print(*idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>']), sep='\\n')\n",
    "\n",
    "    # z1 = torch.randn([args.latent_size]).numpy()\n",
    "    # z2 = torch.randn([args.latent_size]).numpy()\n",
    "    # z = to_var(torch.from_numpy(interpolate(start=z1, end=z2, steps=8)).float())\n",
    "    \n",
    "    batch = next(iter(DataLoader(FastConvert(seqs),batch_size=2,pin_memory=torch.cuda.is_available())))\n",
    "    for k, v in batch.items():\n",
    "        if torch.is_tensor(v):\n",
    "            batch[k] = to_var(v)\n",
    "\n",
    "\n",
    "    \n",
    "    print('-------------Screening procedures based on ESM and Autogluon---------')\n",
    "    num_screen = 0\n",
    "    \n",
    "    screened_seq = []\n",
    "    while(num_screen<5):\n",
    "        # samples_sa, z = model.inference(n=args.num_samples)\n",
    "        logp, mean, logv, z = model(batch['input'], batch['length'])\n",
    "        z = z.cpu().detach().numpy()\n",
    "        z_i = to_var(torch.from_numpy(interpolate(start=z[0], end=z[1], steps=32)).float())\n",
    "    \n",
    "        samples, _ = model.inference(z=z_i)\n",
    "        # samples = torch.cat((samples,samples_sa),0)\n",
    "        \n",
    "        print('-------INTERPOLATION GENERATION-------')\n",
    "        print(*idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>']), sep='\\n')\n",
    "        # temp_data_path = os.path.join(args.data_dir,'results/','temp_generate_'+str(args.num_samples)+'.fasta')\n",
    "        # temp_seqlist = []\n",
    "        # # with open(temp_data_path,'w') as rf:\n",
    "        # #     for i,sp in enumerate(idx2word(samples, i2w=i2w, pad_idx=w2i['<pad>'])):\n",
    "        # #         sp = sp.replace(\" \", \"\").replace(\"<eos>\", \"\")\n",
    "        # #         temp_seqlist.append(sp)\n",
    "        # #         rf.write('>'+str(i)+'\\n'+sp+'\\n')\n",
    "                \n",
    "        # print('-------SCREENing-------')\n",
    "        # predicter = list(screen(temp_data_path))\n",
    "        # selected_seq= [string for flag, string in zip(predicter, temp_seqlist) if flag == 1]\n",
    "        # print(selected_seq)\n",
    "        # screened_seq.extend(selected_seq)\n",
    "        # num_screen = num_screen+predicter.count(1)\n",
    "\n",
    "        break\n",
    "    print(screened_seq)\n",
    "    # with open(os.path.join(args.data_dir,'results/','screen_generate_1000'+'.fasta'),'w') as rf:\n",
    "    #     for i,sp in enumerate(screened_seq):\n",
    "    #         temp_seqlist.append(sp)\n",
    "    #         rf.write('>'+str(i)+'\\n'+sp+'\\n')\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bunch(dict):\n",
    "    def __init__(self, *args, **kwds):\n",
    "        super(Bunch, self).__init__(*args, **kwds)\n",
    "        self.__dict__ = self\n",
    "inference_cfg = Bunch(\n",
    "    \n",
    "    \n",
    "    load_checkpoint = './checkpoints/2023-Dec-05-03:19:06/E499.pytorch',    #The path to the directory where PTB data is stored, and auxiliary data files will be stored.\n",
    "    num_samples = 100,         #生成的新数据量\n",
    "    \n",
    "    data_dir = './data',\n",
    "    max_sequence_length = 150, #Specifies the cut off of long sentences.\n",
    "    embedding_size = 256,\n",
    "    rnn_type = 'gru', #rnn_type Either 'rnn' or 'gru'.\n",
    "    hidden_size = 256, #hidden_size\n",
    "    word_dropout = 0, #word_dropout Word dropout applied to the input of the Decoder which means words will be replaced by <unk> with a probability of word_dropout.\n",
    "    embedding_dropout = 0.3, #embedding_dropout Word embedding dropout applied to the input of the Decoder.\n",
    "\n",
    "    latent_size = 64, #latent_size\n",
    "    num_layers = 1, #num_layers\n",
    "    bidirectional = False, #bidirectional\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./checkpoints/2023-Dec-05-03:19:06/E499.pytorch\n",
      "-------------Screening procedures based on ESM and Autogluon---------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------INTERPOLATION GENERATION-------\n",
      "L P Y L D L E G L Q E Q M E Q L S L M E Q S Q M Y L Q S M L E M Q M F S Y L M E M Q D M M Y E F S Q Y E F L M E Q S Q M E N L S E V F M E E G M Q D L G H S S V G K F M Y D G T Q S V Y F D G E L H S E G V F V E G R T M G K Y S G R T H K G F S E R V K L S T G N G R Y K F T G R G G E K V V T G L N H S\n",
      "L P Y L D L E G L Q E Q M E Q L S L M E Q S Q M Y L Q S M L E M Q M F S Y L M E M Q D M M Y E F S Q Y E F L M E Q S Q M E N L S V N H S V G G R Y E F D L Q Y F S N V M Y D G R T F Y H F G Y F D G R T H S G R G F K F M G D N G S Y H P F D E R F H F E G T R K G S I Y K N E I K N K H R W K M S Q K Y E\n",
      "L P Y L D L E G L Q E Q M E Q L S L M E Q S Q M Y L Q S M L E M Q M F S Y L M E M Q D M M Y E F Y L M E D M G M Y F D G T H M S E Q S Q H F S V M Y D G F Q H F S V Y H F D G E L H S E G F V M T G M Q K Y W D G L P F E F H T D G L H S E F V M I E G F K Q T N S Y H F E G T R K G S I Y K N E I K N K H\n",
      "L P Y L D E G L Q E Q M E Q L S L M E Q S Q M Y L Q S M L E M Q S M E L F M M Y E D M T D E M S Y M P I E M M D A E M M D M M H Y D F Y P G M Y Q P D H S Y F L E G T D Q G L F D G M Q Y F D L G Y N S V F E G D L Q Y I G D N S G R F M Y D G T H R D H S K G F E R F T G M K Q N G S Y V F D N E E H P R\n",
      "L P Y L L L L E Q H G L Q K E F S L E V Q Y F E L P E F D G T M Y Q S V Q M E E G T D L Q Y F M E E G M Q D P G L F D G T H <eos>\n",
      "L P Y L L L L E Q H G L Q K E F S L E V Q Y D E L S V E Y V G M Q T N S Y F V M Y D G T H S S V F G M Y D T V Q Y D G L F D G T H S S Y F T G L F G N H S S Y F A E Y D L I H <eos>\n",
      "L P Y L L L L E Q H G Q K E M N S Y F L E V I M Y Q D E M G M Q T D N S F Y E P M D L M Y E F M D G T Q S Y V F D G H Q S V L G M Y D F Q Y H F D G T H S S Y L P F E D F T G H M K H S S E Q L F N V M Y D G L F D H F V E G T S T H Y D G L F K F K H G F E K T D G L T <eos>\n",
      "L P Y L L L L E Q H G Q K E M N S Y F L P E F I E M Q D M G M H Q T F Y Y D G E R H Q D F M Y D M G P T D H S Y L D G T S Y F H N Q Y F L D G L R N S Y F A E Y D G R M K H Q R K T D S G L F K F S N Y <eos>\n",
      "L P Y L L L E Q H G L Q D E G L K F S V M E Q T D N S Y M P I E F M M E Q D L Q M E G M Q D L Q G Y F L G N Q Y F D L E G L F D G T H M N S Y F D G L E V G L S T D G M K Q G L S T V M N S V Y D G F K F H S T D L H S S G N R K G <eos>\n",
      "L P Y L L L E H L G M Q E Q D L M M Y E D M E F Y Y D E V E Q T D Q S E L G M N Q Y F D L P Y D F M E G T Q S Q Y K N D E G L P F D N E F Y E P R Y I E M D M M T G D N S G F E R A <eos>\n",
      "L P Y L L L E H L M E G Y Q D Q Y F L E D N E Q S V F M E N L E Q D H S S V M E G M Q T Q Y F D L H Y D G L F E K F P G R E F T <eos>\n",
      "L P Y L L L E H L M E G Y Q D Q Y L F E M N D L P E F I M T D S N M M S E Q L P N H S S V M Y F D L E G M Q Y D P G F D H F S T D G K H S T L G L M Y D M G F H Q T F Y Y D H Y D G L F D G T H <eos>\n",
      "L P Y L L L E H M E G L Q D K F E Y L V E D S V M E F Y L P E I E F M D L H E G F K Q G N A V Y S V F G K Y D T E H F G T R K G S I Y K N E I K N K H R W K M S Q K Y E R K K N W D G S Q R K G K K K R L M R Q Q M D S M H Y L D R Y <eos>\n",
      "L P Y L L L E H M E G L Q D K F E Y D L E V M S Y Q T D N S F Y E P R Q D M M T D G H S S Y F H P R Y E D S E S V M G T D S H S S Y F P R G L N S E R N L F D H F K K S E G T <eos>\n",
      "L P Y L L L E H M E I I H Y D N S F Y F E R I T D S N Y Q R S G R E F M T D S H S S Y F P R N H S S Y L P D K R I E N S V M Y E D M G F Y Q H F D H F K Q R N S Y L P K T M I I L F E K N <eos>\n",
      "L P Y L L L E H M E I I H Y D N S F Y F E R N D S Y I E Y D P R S E S H L S E R F M T N G Y D F T I H N K R G L S Q F N V M L S T D S H Y L D R I <eos>\n",
      "L P Y L L L E H M E I I E Y N S Y F N S E L F N E Q H F S V Y L D G H S S V G G R T M E G K Y D L T S I Y K N E R D F K H F G R N E M T D S H Y P R G S E R M T S E N L S K D L S E G R M T D S G K E L S T <eos>\n",
      "L P Y L L L E H M E I I E Y N S Y F N E Q D L S Y Q H F S V M E G I Q Y F L Q Y E D G L M Q Y D F Y H F E G T Q S Q Y K D L S Q G F E R M P I G D A S N R N S E R M R E H L S Q K F N N M M K L R L L I N C F N E R H F V E G I I R Q K F L S T <eos>\n",
      "L P Y L I E M I E S H L S E Q T Y N S F Y P R N E S Q H F S V M Y Q D R I I F S E R M E E G I Q F N Q M D G K H S S Q L F N V M Y D L R I F S Y H F E G R T M Q N G S Q Y K R D E F I E M D G R V Q M E E G D L Q R N S Y Q A D G R N G I R V K M S K G <eos>\n",
      "L P Y L I E M I E S H L S E Q T Y N S F Y P R N H S S V M I Y N R G F D L S I Y V F D N E R H P R Q H F S K E R M P I V I A D N R H S E R M R E N L S Q K A H N M M S L R L L Q H C F N E R N F Y E K R D R Q K D L T R <eos>\n",
      "L P Y L I E M I E S H L S E Q T Q Y N S F Y P R N E S L M D M G M H Q S T F Y Y L D G H N S Q H L S R Q K F N W Y M R G D Q S V M Y D G L F D F Y H F T D G H S L G N Q Y F D L H S E R F T <eos>\n",
      "L P Y L I E M I E S H L S E Q T Q Y N S F Y L P E F N S V M Y D G L F H F T K Q N H S S Y L P L F N E I I F Y E Y K D R I T T M V N A Y M D M E P K H G M K Q A Q M E Q Q K S Q Q M N K L V R D F I E E D M G A D G N N Y Q R H D D S K F E M D R L F G D Q R N K L S E <eos>\n",
      "L P Y L I H M E I E M I E S M E L Q N S F Y F M E E L G I Q N V M L E D M G M T T D H Y D F Y H F G T Q Y K R W E D M G T Q S Y V F D N E E H P R Q H Y D F Y H F E G T R K G S I Y K N E I K N K H R W K M S Q K Y E R K K N W D G S Q R K G K K K R L M R Q Q M D S M H Y L D R Y <eos>\n",
      "L P Y L I H M E I E M I E S M E L Q N H S S F V M I Y D G R E M D Q S G L F D F M Y D G T H D S S G L F T <eos>\n",
      "L P I L M E I E H Y D N S Y F D L Y N E S D P R S E F N S V M Y D G R T M G R Y I G D R G S Q K N H S S R E F M P I G D A S N R N S E R M R E H L S Q K F N N M M K L R L L I N C F N E R H F V E G I I R Q K F L S T <eos>\n",
      "L P I L M E I E M S H L Y N S F Y F E Q H F S V Q T Y N L D E S V G L T S I Y F N W E R N L S Q K H N W M M K D L S E R M E E G P R Q H S S Y F L V E G R T M T G D A K M S G R T M N Y R M D H Y H P R D E K F H T D G L P K N H S E R L F N Q Y K N D L S E R A <eos>\n",
      "L P I L H M E I E M I E I S H Y L S E F N Q Y F N V M L I E G D Q S V F M Y L D T E H L M G D Q R T S Y M P I G D A V I M N D M M K H S E G R T M T D H M G R M K G E D L S E G T R T M Q G K N G R T M E Q S K M S E G R T M Y N R M E D M H K K Y D L F K H S E G R T M T D H M K G R M E E G T R Y I H G\n",
      "L P I L H M E I E M I E I S H Y L S E F N Q Y F N V M L I E G D L Q R Y Q D R I T T D M G G P S E H L S E Q Q H F S V Q L N V I Y D A E S K D L S E R M E E G D L Q R K F K R N G T R T S Y H M R E G I I R V Q M K K D L S Q K E M S E F W E R M T H Y D H R K T D H S G R T M G K N G R T H Y K G N L R T\n",
      "L P I L H M E I E M I E I S H Y L S E F N Q Y F N V M L I E G D L Q R Y Q D R I T T D M G G P S E H L S E Q Q H F S V Q L N V I Y D A E S K D L S E R M E E G D L Q R K F K R N G T R T S Y H M R E G I I R V Q M K K D L S Q K E M S E F W E R M T H Y D H R K T D H S G R T M G K N G R T H Y K G N L R T\n",
      "L P I H I M E I H I M E E I S H N S Q F L M E D S Q H F S Y F M L E G T D N S I Y Q D L S Q Y K R N W E G M R Q D L Q Y F M N G R Y D T I D H G S R G K Q N V M K L S T R H L D S E V Q L K Y N D R C F N R E N S V Q K Y K N H S S V Q L H S S V G R K K K V V R K M S K V V M S Y M V Q Y K D E S K L E S\n",
      "L P I H I M E I H I N S D L K S N H S S F V M I Y D G R E M D Q S G L F D Q H F S T Q L V G K Y D L R S I Y K A N R N S E R M R E H L S Q K F N N M M K L R L L I N C F N E R H F V E G I I R Q K F L S T <eos>\n",
      "L P I H I M E I H I N S D L F K N H S S H F Y P R Y I E N S V M D G R T M Q N G S R Q K H N S R M Q N D L S E R F M Y D G R T M G K H G D S E R M E E P I V F N S N H L P R K Q N A S L D E L K K S V G K K V V R M S K L V S V G K Y D L T S K E S V G G R E I T D K F K R G <eos>\n",
      "L P I H I M E I H I N S D L F K N H S S H F Y P R Y I E N S V M D G R T M Q N G S R Q K H N S R M Q N D L S E R F M Y D G R T M G K H G D S E R M E E P I V F N S N H L P R K Q N A S L D E L K K S V G K K V V R M S K L V S V G K Y D L T S K E R A S I M E K A E D M M K D K S K E G F E K R Q K L S T <eos>\n",
      "L P I H I M E H I D N S Y F D N E E H S Q L F Y P R Y I E D M M T S Y Q M D G R N Q M D H S K G L R L Q Y D R I T T M V N A Y M D M E P K H G M K Q A Q M E Q Q K S Q Q M N K L V R D F I E E D M G A D G N N Y Q R H D D S K F E M D R L F G D Q R N K L S E <eos>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "main(inference_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "z1 = torch.randn([100]).numpy()\n",
    "z2 = torch.randn([100]).numpy()\n",
    "z1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utlis.dataset import FastConvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = ['KKRPKPGGWNTGGSRYPGQGSPGGNRYPPQGGGGWGQPHGGGWGQPHGGGWGQPHGGGWGQPHGGGWGQGGGTHSQWNKPSKPKTNMKHMAGAAAAGAVVGGLGGYMLGSAMSRPIIHFGSD',\n",
    "       'SKGPGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGRGDSPYSGY'\n",
    "       ]\n",
    "test_data = FastConvert(sequence_strs=seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  7,  7, 20, 14,  7, 14, 11, 11, 22, 19, 18, 11, 11, 17, 20, 16, 14,\n",
      "         11, 13, 11, 17, 14, 11, 11, 19, 20, 16, 14, 14, 13, 11, 11, 11, 11, 22,\n",
      "         11, 13, 14,  8, 11, 11, 11, 22, 11, 13, 14,  8, 11, 11, 11, 22, 11, 13,\n",
      "         14,  8, 11, 11, 11, 22, 11, 13, 14,  8, 11, 11, 11, 22, 11, 13, 11, 11,\n",
      "         11, 18,  8, 17, 13, 22, 19,  7, 14, 17,  7, 14,  7, 18, 19,  4,  7,  8,\n",
      "          4,  5, 11,  5,  5,  5,  5, 11,  5, 15, 15, 11, 11,  9, 11, 11, 16,  4,\n",
      "          9, 11, 17,  5,  4, 17, 20, 14, 21, 21,  8, 10, 11, 17, 12,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 2, 17,  7, 11, 14, 11, 20, 11, 12, 17, 14, 16, 17, 11, 20, 11, 12, 17,\n",
      "         14, 16, 17, 11, 20, 11, 12, 17, 14, 16, 17, 11, 20, 11, 12, 17, 14, 16,\n",
      "         17, 11, 20, 11, 12, 17, 14, 16, 17, 11, 20, 11, 12, 17, 14, 16, 17, 11,\n",
      "         20, 11, 12, 17, 14, 16, 17, 11, 20, 11, 12, 17, 14, 16, 17, 11, 20, 11,\n",
      "         12, 17, 14, 16, 17, 11, 20, 11, 12, 17, 14, 16, 17, 11, 20, 11, 12, 17,\n",
      "         14, 16, 17, 11, 20, 11, 12, 17, 14, 16, 17, 11, 20, 11, 12, 17, 14, 16,\n",
      "         17, 11, 20, 11, 12, 17, 14, 16, 17, 11, 20, 11, 12, 17, 14, 16, 17, 11,\n",
      "         20, 11, 12, 17, 14, 16, 17, 11, 20, 11, 12, 17, 14, 16, 17, 11, 20, 11,\n",
      "         12, 17, 14, 16, 17, 11, 20, 11, 12, 17, 14, 16, 17, 11, 20, 11, 12, 17,\n",
      "         14, 16, 17, 11, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_loader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=2,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "for batch in data_loader:\n",
    "    print(batch['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Xihe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
