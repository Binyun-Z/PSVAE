{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/results/2023-Dec-05-04:14:26_generate_10.fasta\" # Path to data\n",
    "EMB_PRE_PATH = \"/home/bli/.cache/torch/hub/checkpoints/esm1b_t33_650M_UR50S.pt\" \n",
    "EMBED_PATH ='./ESM_embed/'\n",
    "EMB_LAYER = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bli/.conda/envs/ag/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from esm import Alphabet, FastaBatchedDataset, ProteinBertModel, pretrained, MSATransformer\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "class ProteinExtractionParams:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_location=EMB_PRE_PATH,\n",
    "        fasta_file = None,\n",
    "        csv_file = None,\n",
    "        output_dir = None,\n",
    "        toks_per_batch=10,\n",
    "        repr_layers=[-1],\n",
    "        include='mean',\n",
    "        truncation_seq_length=512,\n",
    "        nogpu=False,\n",
    "    ):\n",
    "        self.model_location = model_location\n",
    "        self.fasta_file = fasta_file\n",
    "        self.csv_file = csv_file\n",
    "\n",
    "        self.output_dir = pathlib.Path(output_dir)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        self.toks_per_batch = toks_per_batch\n",
    "        self.repr_layers = repr_layers\n",
    "        self.include = include\n",
    "        self.truncation_seq_length = truncation_seq_length\n",
    "        self.nogpu = nogpu\n",
    "\n",
    "\n",
    "def run(args):\n",
    "    model, alphabet = pretrained.load_model_and_alphabet(args.model_location)\n",
    "    model.eval()\n",
    "    if isinstance(model, MSATransformer):\n",
    "        raise ValueError(\n",
    "            \"This script currently does not handle models with MSA input (MSA Transformer).\"\n",
    "        )\n",
    "    if torch.cuda.is_available() and not args.nogpu:\n",
    "        model = model.cuda()\n",
    "        print(\"Transferred model to GPU\")\n",
    "\n",
    "\n",
    "    if(args.fasta_file):\n",
    "        dataset = FastaBatchedDataset.from_file(args.fasta_file)\n",
    "        batches = dataset.get_batch_indices(args.toks_per_batch, extra_toks_per_seq=1)\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, collate_fn=alphabet.get_batch_converter(args.truncation_seq_length), batch_sampler=batches\n",
    "        )\n",
    "        print(f\"Read {args.fasta_file} with {len(dataset)} sequences\")\n",
    "    elif(args.csv_file):\n",
    "        data_df = pd.read_csv(args.csv_file)\n",
    "        \n",
    "        protein_id = data_df['id']\n",
    "        # class FastaBatchedDataset(object):\n",
    "        #     def __init__(self, sequence_labels, sequence_strs):\n",
    "        #         self.sequence_labels = list(sequence_labels)\n",
    "        #         self.sequence_strs = list(sequence_strs)\n",
    "        dataset = FastaBatchedDataset(data_df['id'],data_df['seq'])\n",
    "        batches = dataset.get_batch_indices(args.toks_per_batch, extra_toks_per_seq=1)\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            dataset, collate_fn=alphabet.get_batch_converter(args.truncation_seq_length), batch_sampler=batches\n",
    "        )\n",
    "        print(f\"Read {args.csv_file} with {len(dataset)} sequences\")\n",
    "    else:\n",
    "        print('no file!')\n",
    "\n",
    "    args.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return_contacts = \"contacts\" in args.include                                                                                                                                \n",
    "\n",
    "    assert all(-(model.num_layers + 1) <= i <= model.num_layers for i in args.repr_layers)\n",
    "    repr_layers = [(i + model.num_layers + 1) % (model.num_layers + 1) for i in args.repr_layers]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "            print(\n",
    "                f\"Processing {batch_idx + 1} of {len(batches)} batches ({toks.size(0)} sequences)\"\n",
    "            )\n",
    "            if torch.cuda.is_available() and not args.nogpu:\n",
    "                toks = toks.to(device=\"cuda\", non_blocking=True)\n",
    "\n",
    "            out = model(toks, repr_layers=repr_layers, return_contacts=return_contacts)\n",
    "\n",
    "            logits = out[\"logits\"].to(device=\"cpu\")\n",
    "            representations = {\n",
    "                layer: t.to(device=\"cpu\") for layer, t in out[\"representations\"].items()\n",
    "            }\n",
    "            if return_contacts:\n",
    "                contacts = out[\"contacts\"].to(device=\"cpu\")\n",
    "\n",
    "            for i, label in enumerate(labels):\n",
    "                args.output_file = args.output_dir / f\"{label}.pt\"\n",
    "                args.output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                result = {\"label\": label}\n",
    "                truncate_len = min(args.truncation_seq_length, len(strs[i]))\n",
    "                # Call clone on tensors to ensure tensors are not views into a larger representation\n",
    "                # See https://github.com/pytorch/pytorch/issues/1995\n",
    "                if \"per_tok\" in args.include:\n",
    "                    result[\"representations\"] = {\n",
    "                        layer: t[i, 1 : truncate_len + 1].clone()\n",
    "                        for layer, t in representations.items()\n",
    "                    }\n",
    "                if \"mean\" in args.include:\n",
    "                    result[\"mean_representations\"] = {\n",
    "                        layer: t[i, 1 : truncate_len + 1].mean(0).clone()\n",
    "                        for layer, t in representations.items()\n",
    "                    }\n",
    "                if \"bos\" in args.include:\n",
    "                    result[\"bos_representations\"] = {\n",
    "                        layer: t[i, 0].clone() for layer, t in representations.items()\n",
    "                    }\n",
    "                if return_contacts:\n",
    "                    result[\"contacts\"] = conacts[i, : truncate_len, : truncate_len].clone()\n",
    "\n",
    "                torch.save(\n",
    "                    result,\n",
    "                    args.output_file,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embed(data_file):\n",
    "    input_data  =DATA_PATH\n",
    "    output_dir = EMBED_PATH\n",
    "    try:\n",
    "        # 创建文件夹\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Folder '{output_dir}' has been created.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Folder '{output_dir}' already exists.\")\n",
    "    args = ProteinExtractionParams(fasta_file=input_data,output_dir=output_dir)\n",
    "    run(args)\n",
    "    print('Extract ESM embeddings for {}, save in {}'.format(input_data,output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './ESM_embed/' already exists.\n",
      "Transferred model to GPU\n",
      "Read ../data/results/2023-Dec-05-04:14:26_generate_10.fasta with 10 sequences\n",
      "Processing 1 of 10 batches (1 sequences)\n",
      "Processing 2 of 10 batches (1 sequences)\n",
      "Processing 3 of 10 batches (1 sequences)\n",
      "Processing 4 of 10 batches (1 sequences)\n",
      "Processing 5 of 10 batches (1 sequences)\n",
      "Processing 6 of 10 batches (1 sequences)\n",
      "Processing 7 of 10 batches (1 sequences)\n",
      "Processing 8 of 10 batches (1 sequences)\n",
      "Processing 9 of 10 batches (1 sequences)\n",
      "Processing 10 of 10 batches (1 sequences)\n",
      "Extract ESM embeddings for ../data/results/2023-Dec-05-04:14:26_generate_10.fasta, save in ./ESM_embed/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "extract_embed(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_esm_embed(EMBED_PATH):\n",
    "\n",
    "    EMB_LAYER = 33\n",
    "    Xs = []\n",
    "\n",
    "    for file in os.listdir(EMBED_PATH):\n",
    "        fn = f'{EMBED_PATH}/{file}'\n",
    "        embs = torch.load(fn)\n",
    "        \n",
    "        Xs.append(embs['mean_representations'][EMB_LAYER])\n",
    "\n",
    "    Xs = torch.stack(Xs, dim=0).numpy()\n",
    "    print('load esm embedding')\n",
    "\n",
    "    return Xs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load esm embedding\n"
     ]
    }
   ],
   "source": [
    "embed_temp = load_esm_embed(EMBED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1270</th>\n",
       "      <th>1271</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>1275</th>\n",
       "      <th>1276</th>\n",
       "      <th>1277</th>\n",
       "      <th>1278</th>\n",
       "      <th>1279</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.167233</td>\n",
       "      <td>0.130327</td>\n",
       "      <td>-0.168505</td>\n",
       "      <td>0.030995</td>\n",
       "      <td>-0.039343</td>\n",
       "      <td>-0.089550</td>\n",
       "      <td>-0.243623</td>\n",
       "      <td>-0.142271</td>\n",
       "      <td>-0.119898</td>\n",
       "      <td>0.069863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125417</td>\n",
       "      <td>0.020173</td>\n",
       "      <td>-0.048031</td>\n",
       "      <td>0.214600</td>\n",
       "      <td>-1.258521</td>\n",
       "      <td>-0.075790</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.013886</td>\n",
       "      <td>0.198329</td>\n",
       "      <td>0.207286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.142538</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>-0.095545</td>\n",
       "      <td>0.114190</td>\n",
       "      <td>0.041693</td>\n",
       "      <td>-0.176500</td>\n",
       "      <td>-0.208702</td>\n",
       "      <td>-0.056122</td>\n",
       "      <td>-0.150602</td>\n",
       "      <td>-0.115460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132358</td>\n",
       "      <td>0.097449</td>\n",
       "      <td>-0.016638</td>\n",
       "      <td>0.017523</td>\n",
       "      <td>-1.032016</td>\n",
       "      <td>-0.104907</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.187973</td>\n",
       "      <td>0.164439</td>\n",
       "      <td>0.159598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.166239</td>\n",
       "      <td>0.100672</td>\n",
       "      <td>-0.163813</td>\n",
       "      <td>0.078611</td>\n",
       "      <td>0.019763</td>\n",
       "      <td>-0.116436</td>\n",
       "      <td>-0.133373</td>\n",
       "      <td>-0.009965</td>\n",
       "      <td>-0.058678</td>\n",
       "      <td>0.031321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137263</td>\n",
       "      <td>-0.009128</td>\n",
       "      <td>-0.049559</td>\n",
       "      <td>0.120188</td>\n",
       "      <td>-1.012077</td>\n",
       "      <td>-0.077144</td>\n",
       "      <td>0.126060</td>\n",
       "      <td>0.010027</td>\n",
       "      <td>0.176264</td>\n",
       "      <td>0.155619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028822</td>\n",
       "      <td>0.143447</td>\n",
       "      <td>-0.191251</td>\n",
       "      <td>0.218951</td>\n",
       "      <td>-0.138921</td>\n",
       "      <td>-0.007613</td>\n",
       "      <td>-0.143106</td>\n",
       "      <td>-0.041320</td>\n",
       "      <td>-0.072974</td>\n",
       "      <td>-0.038920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167929</td>\n",
       "      <td>-0.110380</td>\n",
       "      <td>-0.073190</td>\n",
       "      <td>0.170297</td>\n",
       "      <td>-0.821683</td>\n",
       "      <td>-0.033038</td>\n",
       "      <td>0.023359</td>\n",
       "      <td>-0.108370</td>\n",
       "      <td>-0.054460</td>\n",
       "      <td>0.023952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.117276</td>\n",
       "      <td>0.050493</td>\n",
       "      <td>-0.087267</td>\n",
       "      <td>0.177359</td>\n",
       "      <td>-0.047596</td>\n",
       "      <td>-0.171305</td>\n",
       "      <td>-0.322699</td>\n",
       "      <td>0.062665</td>\n",
       "      <td>-0.029130</td>\n",
       "      <td>-0.088583</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049856</td>\n",
       "      <td>0.120746</td>\n",
       "      <td>-0.132633</td>\n",
       "      <td>0.104556</td>\n",
       "      <td>-1.391821</td>\n",
       "      <td>0.102206</td>\n",
       "      <td>0.010904</td>\n",
       "      <td>0.151753</td>\n",
       "      <td>0.136671</td>\n",
       "      <td>0.025402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.112103</td>\n",
       "      <td>0.153404</td>\n",
       "      <td>-0.139760</td>\n",
       "      <td>0.124497</td>\n",
       "      <td>-0.136296</td>\n",
       "      <td>-0.074639</td>\n",
       "      <td>-0.157331</td>\n",
       "      <td>-0.027425</td>\n",
       "      <td>-0.050942</td>\n",
       "      <td>0.061439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129701</td>\n",
       "      <td>-0.052547</td>\n",
       "      <td>-0.064770</td>\n",
       "      <td>0.175037</td>\n",
       "      <td>-1.251352</td>\n",
       "      <td>0.041543</td>\n",
       "      <td>0.146135</td>\n",
       "      <td>0.041446</td>\n",
       "      <td>0.105533</td>\n",
       "      <td>0.136077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.208613</td>\n",
       "      <td>0.230555</td>\n",
       "      <td>-0.153985</td>\n",
       "      <td>0.193166</td>\n",
       "      <td>-0.014747</td>\n",
       "      <td>-0.149238</td>\n",
       "      <td>-0.326040</td>\n",
       "      <td>-0.134078</td>\n",
       "      <td>-0.193030</td>\n",
       "      <td>-0.023235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118994</td>\n",
       "      <td>-0.015515</td>\n",
       "      <td>-0.017523</td>\n",
       "      <td>0.049031</td>\n",
       "      <td>-1.140906</td>\n",
       "      <td>0.021154</td>\n",
       "      <td>-0.052373</td>\n",
       "      <td>-0.030905</td>\n",
       "      <td>0.135686</td>\n",
       "      <td>0.227094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.184214</td>\n",
       "      <td>0.109668</td>\n",
       "      <td>-0.151756</td>\n",
       "      <td>0.121802</td>\n",
       "      <td>-0.060418</td>\n",
       "      <td>-0.080945</td>\n",
       "      <td>-0.140876</td>\n",
       "      <td>-0.017313</td>\n",
       "      <td>-0.079954</td>\n",
       "      <td>0.078913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048046</td>\n",
       "      <td>0.055762</td>\n",
       "      <td>-0.103678</td>\n",
       "      <td>0.083915</td>\n",
       "      <td>-1.258884</td>\n",
       "      <td>-0.029053</td>\n",
       "      <td>0.100055</td>\n",
       "      <td>-0.004953</td>\n",
       "      <td>0.133810</td>\n",
       "      <td>0.107471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.150803</td>\n",
       "      <td>0.097124</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>0.142981</td>\n",
       "      <td>-0.033813</td>\n",
       "      <td>-0.106572</td>\n",
       "      <td>-0.208770</td>\n",
       "      <td>-0.066382</td>\n",
       "      <td>-0.096352</td>\n",
       "      <td>0.050012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014088</td>\n",
       "      <td>0.018893</td>\n",
       "      <td>-0.085045</td>\n",
       "      <td>0.119966</td>\n",
       "      <td>-1.387731</td>\n",
       "      <td>-0.031153</td>\n",
       "      <td>0.086471</td>\n",
       "      <td>0.080461</td>\n",
       "      <td>0.093710</td>\n",
       "      <td>0.135901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.193531</td>\n",
       "      <td>0.130220</td>\n",
       "      <td>-0.190075</td>\n",
       "      <td>0.108972</td>\n",
       "      <td>0.046108</td>\n",
       "      <td>-0.163080</td>\n",
       "      <td>-0.299659</td>\n",
       "      <td>-0.121659</td>\n",
       "      <td>-0.151458</td>\n",
       "      <td>-0.031004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052929</td>\n",
       "      <td>0.016717</td>\n",
       "      <td>-0.051666</td>\n",
       "      <td>0.112521</td>\n",
       "      <td>-1.235597</td>\n",
       "      <td>-0.018505</td>\n",
       "      <td>-0.045528</td>\n",
       "      <td>0.012520</td>\n",
       "      <td>0.091805</td>\n",
       "      <td>0.215697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.167233  0.130327 -0.168505  0.030995 -0.039343 -0.089550 -0.243623   \n",
       "1  0.142538  0.001049 -0.095545  0.114190  0.041693 -0.176500 -0.208702   \n",
       "2  0.166239  0.100672 -0.163813  0.078611  0.019763 -0.116436 -0.133373   \n",
       "3  0.028822  0.143447 -0.191251  0.218951 -0.138921 -0.007613 -0.143106   \n",
       "4  0.117276  0.050493 -0.087267  0.177359 -0.047596 -0.171305 -0.322699   \n",
       "5  0.112103  0.153404 -0.139760  0.124497 -0.136296 -0.074639 -0.157331   \n",
       "6  0.208613  0.230555 -0.153985  0.193166 -0.014747 -0.149238 -0.326040   \n",
       "7  0.184214  0.109668 -0.151756  0.121802 -0.060418 -0.080945 -0.140876   \n",
       "8  0.150803  0.097124 -0.152745  0.142981 -0.033813 -0.106572 -0.208770   \n",
       "9  0.193531  0.130220 -0.190075  0.108972  0.046108 -0.163080 -0.299659   \n",
       "\n",
       "       7         8         9     ...      1270      1271      1272      1273  \\\n",
       "0 -0.142271 -0.119898  0.069863  ...  0.125417  0.020173 -0.048031  0.214600   \n",
       "1 -0.056122 -0.150602 -0.115460  ...  0.132358  0.097449 -0.016638  0.017523   \n",
       "2 -0.009965 -0.058678  0.031321  ...  0.137263 -0.009128 -0.049559  0.120188   \n",
       "3 -0.041320 -0.072974 -0.038920  ...  0.167929 -0.110380 -0.073190  0.170297   \n",
       "4  0.062665 -0.029130 -0.088583  ... -0.049856  0.120746 -0.132633  0.104556   \n",
       "5 -0.027425 -0.050942  0.061439  ...  0.129701 -0.052547 -0.064770  0.175037   \n",
       "6 -0.134078 -0.193030 -0.023235  ...  0.118994 -0.015515 -0.017523  0.049031   \n",
       "7 -0.017313 -0.079954  0.078913  ...  0.048046  0.055762 -0.103678  0.083915   \n",
       "8 -0.066382 -0.096352  0.050012  ...  0.014088  0.018893 -0.085045  0.119966   \n",
       "9 -0.121659 -0.151458 -0.031004  ...  0.052929  0.016717 -0.051666  0.112521   \n",
       "\n",
       "       1274      1275      1276      1277      1278      1279  \n",
       "0 -1.258521 -0.075790  0.121951  0.013886  0.198329  0.207286  \n",
       "1 -1.032016 -0.104907  0.002389  0.187973  0.164439  0.159598  \n",
       "2 -1.012077 -0.077144  0.126060  0.010027  0.176264  0.155619  \n",
       "3 -0.821683 -0.033038  0.023359 -0.108370 -0.054460  0.023952  \n",
       "4 -1.391821  0.102206  0.010904  0.151753  0.136671  0.025402  \n",
       "5 -1.251352  0.041543  0.146135  0.041446  0.105533  0.136077  \n",
       "6 -1.140906  0.021154 -0.052373 -0.030905  0.135686  0.227094  \n",
       "7 -1.258884 -0.029053  0.100055 -0.004953  0.133810  0.107471  \n",
       "8 -1.387731 -0.031153  0.086471  0.080461  0.093710  0.135901  \n",
       "9 -1.235597 -0.018505 -0.045528  0.012520  0.091805  0.215697  \n",
       "\n",
       "[10 rows x 1280 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(embed_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "6    0\n",
       "7    0\n",
       "8    0\n",
       "9    0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset,TabularPredictor\n",
    "import pandas as pd\n",
    "predicter = TabularPredictor.load('./AutoML_ESM/')\n",
    "y_pred = predicter.predict(pd.DataFrame(embed_temp))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'e']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y_pred)\n",
    "b = ['a','b','c','d','e','f']\n",
    "a = [1,1,0,0,1,0]\n",
    "selected_seq= [string for flag, string in zip(a, b) if flag == 1]\n",
    "selected_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Xihe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
